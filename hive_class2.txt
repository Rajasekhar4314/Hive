








### WORKING WITH EXTERNAL TABLES

creating a file
vi department_data.txt

Copying file to hdfs:
hdfs dfs -put /tmp/hive_class/department_data.txt /tmp/hive_class;

checking data if available:
hdfs dfs -ls  /tmp/hive_class/

creating External Table:

create external table dept_details_external
    > (
    > dept_id int,
    > dept_name string,
    > manager_id int,
    > salary int
    > )
    > row format delimited
    > fields terminated by ','
    > location '/tmp/hive_class/';

select * from dept_details_external;



### WORKING WITH ARRAY DATA

Creating a file in Cloudera:

vi array_data.csv

create table in Hive:

create table employee
    > (
    > id int,
    > name string,
    > skills array<string>
    > )
    > row format delimited
    > fields terminated by ","
    > collection items terminated by ":";

Load data into table:

load data local inpath "file:///tmp/big_data/hive/array_data.csv" into table employee;

Querying a table:

select id, name, skills[1] from employee;

select id, name, size(skills) from employee;

select id,
    > name,
    > array_contains(skills, "hadoop") as knows_hadoop,
    > sort_array(skills)
    > from employee;




### WORKING WITH MAP DATA

creating a file in cloudera:
vi map_data.csv

Copying file to tmp directory:
cp data/map_data.csv /tmp/big_data/hive

Creating a table in hive:

create table employee_map_data
    > (
    > id int,
    > name string,
    > details map<string,string>
    > )
    > row format delimited
    > fields terminated by ","
    > collection items terminated by "|"
    > map keys terminated by ":";


Loading data:

load data local inpath "file:///tmp/big_data/hive/map_data.csv" into table employee_map_data;

Quering a table:

set hive.cli.print.header = true;
select * from employee_map_data;

Playing some fun with data:

select
    > id,
    > name,
    > details,
    > size(details),
    > map_keys(details) as map_keys_distinct,
    > map_values(details) as map_values_distinct
    > from employee_map_data;




