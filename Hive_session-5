# Create multiply udf file
vim multiply_udf.py

## Add py file into hive
add file /tmp/big_data/hive/multiply_udf.py

# querying 
select transform (quantityordered) using 'python multiply_udf.py' as (quantity int) from sales_order_data_orc limit 5;


# Create multiply udf file
vim many_column_udf.py

## Add py file into hive
add file /tmp/big_data/hive/many_column_udf.py

# querying 
select transform (country, quantityordered) using 'many_column_udf.py' as (country string, quantity int) from sales_order_data_orc limit 5;


# use below hive statements for bucketing

hive> create table users                                                                                                                      
    > (                                                                                                                                       
    > id int,                                                                                                                                 
    > name string,                                                                                                                            
    > salary int,                                                                                                                             
    > unit string                                                                                                                             
    > )row format delimited                                                                                                                   
    > fields terminated by ','; 
  
load data local inpath 'file:///tmp/hive_class/users.csv' into table users;
    
hive> create table locations                                                                                                                  
    > (                                                                                                                                       
    > id int,                                                                                                                                 
    > location string                                                                                                                         
    > )                                                                                                                                       
    > row format delimited                                                                                                                    
    > fields terminated by ','; 
    
load data local inpath 'file:///tmp/hive_class/locations.csv' into table locations; 

set hive.enforce.bucketing=true;
    
    
 hive> create table buck_users                                                                                                                 
    > (                                                                                                                                       
    > id int,                                                                                                                                 
    > name string,                                                                                                                            
    > salary int,                                                                                                                             
    > unit string                                                                                                                             
    > )                                                                                                                                       
    > clustered by (id)                                                                                                                       
    > sorted by (id)                                                                                                                          
    > into 2 buckets;
    
insert overwrite table buck_users select * from users;
    
hive> create table buck_locations                                                                                                             
    > (                                                                                                                                       
    > id int,                                                                                                                                 
    > location string                                                                                                                         
    > )                                                                                                                                       
    > clustered by (id)                                                                                                                       
    > sorted by (id)                                                                                                                          
    > into 2 buckets; 
    
 insert overwrite table buck_locations select * from locations;
 
 # Checking buckets in warehouse
 
 hdfs dfs -ls /user/hive/warehouse/hive_class2.db/buck_locations
 hdfs dfs -ls /user/hive/warehouse/hive_class2.db/buck_users
 
 
 ------------------------------------------------------------------------------------------------------------------------------
Reduce-Side Join
------------------------------------------------------------------------------------------------------------------------------

SET hive.auto.convert.join=false;
SELECT * FROM buck_users u INNER JOIN buck_locations l ON u.id = l.id;

Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

------------------------------------------------------------------------------------------------------------------------------
Map Side Join
------------------------------------------------------------------------------------------------------------------------------

SET hive.auto.convert.join=true;
SELECT * FROM buck_users u INNER JOIN buck_locations l ON u.id = l.id;

Mapred Local Task Succeeded . Convert the Join into MapJoin
Number of reduce tasks is set to 0 since there's no reduce operator
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0

------------------------------------------------------------------------------------------------------------------------------
Bucket Map Join
------------------------------------------------------------------------------------------------------------------------------
set hive.optimize.bucketmapjoin=true;
SET hive.auto.convert.join=true;

SELECT * FROM buck_users u INNER JOIN buck_locations l ON u.id = l.id;

------------------------------------------------------------------------------------------------------------------------------
Sorted Merge Bucket Map Join
------------------------------------------------------------------------------------------------------------------------------
set hive.enforce.sortmergebucketmapjoin=false;
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;


SET hive.auto.convert.join=false;
SELECT * FROM buck_users u INNER JOIN buck_locations l ON u.id = l.id;

No MapLocal Task to create hash table.
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 0
 
